{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a30cc1f7",
   "metadata": {},
   "source": [
    "## **Extraction of the transcripts**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e00ff1",
   "metadata": {},
   "source": [
    "##### Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36709f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import openai\n",
    "import tiktoken\n",
    "import pinecone\n",
    "\n",
    "from openai import OpenAI\n",
    "from uuid import uuid4\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b9db67",
   "metadata": {},
   "source": [
    "##### In this first part we are going to colect our data from the Youtube videos. We get the transcripts and name them with the video title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6903027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcript saved: transcripts\\Master_Your_Sleep_lIo9FcrljDk.txt\n",
      " Transcript saved: transcripts\\Your_Diet_is_Changing_Your_Brain_NbymuYEEqlE.txt\n",
      " Transcript saved: transcripts\\Foods_Control_Our_Moods_Q4qWzbP0q7I.txt\n",
      " Transcript saved: transcripts\\4_Small_Habits_mMHNvy9pFj0.txt\n",
      " Transcript saved: transcripts\\Young_Forever_gO_x3gnXBzg.txt\n",
      " Transcript saved: transcripts\\Powerful_Happiness_Hacks_14-DJFPm1_4.txt\n"
     ]
    }
   ],
   "source": [
    "# List of videos\n",
    "video_data = [\n",
    "    {\"url\": \"https://www.youtube.com/watch?v=lIo9FcrljDk\", \"Name\":\"Master Your Sleep\"},\n",
    "    {\"url\": \"https://www.youtube.com/watch?v=NbymuYEEqlE\", \"Name\":\"Your Diet is Changing Your Brain\"},\n",
    "    {\"url\": \"https://www.youtube.com/watch?v=Q4qWzbP0q7I\", \"Name\":\"Foods Control Our Moods\"},\n",
    "    {\"url\": \"https://www.youtube.com/watch?v=mMHNvy9pFj0\", \"Name\": \"4 Small Habits\"},\n",
    "    {\"url\": \"https://www.youtube.com/watch?v=gO_x3gnXBzg\", \"Name\": \"Young Forever\"},\n",
    "    {\"url\": \"https://www.youtube.com/watch?v=14-DJFPm1_4\", \"Name\": \"Powerful Happiness Hacks\"},\n",
    "]\n",
    "\n",
    "# extract video ID from YouTube URL\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r\"v=([a-zA-Z0-9_-]{11})\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# clean transcript text\n",
    "def clean_text(transcript):\n",
    "    return \" \".join([entry['text'] for entry in transcript]).replace(\"\\n\", \" \").strip()\n",
    "\n",
    "output_folder = \"transcripts\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# save transcripts to text files\n",
    "for video in video_data:\n",
    "    video_id = extract_video_id(video['url'])\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en']) \n",
    "        cleaned = clean_text(transcript)\n",
    "\n",
    "        filename = f\"{video['Name'].replace(' ', '_')}_{video_id}.txt\"\n",
    "        file_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(cleaned)\n",
    "\n",
    "        print(f\" Transcript saved: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error with {video['url']}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6c743",
   "metadata": {},
   "source": [
    "##### Now, that we have the transcripts. We need to **chunk** the text to get more friemndly embedings for our vector store.\n",
    "We use the timestamps of the videos so we get meaningful chunks, each one already tied to a specific topic.\n",
    "This is  more accurate, and much easier to label, search, and structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564095e4",
   "metadata": {},
   "source": [
    "First video: Master your sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32645147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Segmented chunks saved to: c:\\Users\\alvar\\OneDrive\\文件\\Iron Hack\\ALMA-Chatbot\\Chunks\\Master_Your_Sleep_lIo9FcrljDk.json\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions ===\n",
    "\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r\"v=([a-zA-Z0-9_-]{11})\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def hms_to_seconds(hms: str) -> float:\n",
    "    h, m, s = map(int, hms.split(':'))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "# Video Info and Timestamp Segments \n",
    "\n",
    "video = {\n",
    "    \"url\": \"https://www.youtube.com/watch?v=lIo9FcrljDk\",\n",
    "    \"Name\": \"Master Your Sleep\"\n",
    "}\n",
    "video_id = extract_video_id(video[\"url\"])\n",
    "video_title = video[\"Name\"]\n",
    "\n",
    "timestamp_segments = [\n",
    "    {\"start\": \"00:00:00\", \"end\": \"00:01:11\", \"topic\": \"Introduction to Sleep & Wakefulness\"},\n",
    "    {\"start\": \"00:01:11\", \"end\": \"00:03:30\", \"topic\": \"The Science of Sleep: Adenosine Explained\"},\n",
    "    {\"start\": \"00:03:30\", \"end\": \"00:05:08\", \"topic\": \"Circadian Rhythms: The Body’s Internal Clock\"},\n",
    "    {\"start\": \"00:05:08\", \"end\": \"00:10:16\", \"topic\": \"The Role of Cortisol & Melatonin\"},\n",
    "    {\"start\": \"00:10:16\", \"end\": \"00:14:12\", \"topic\": \"Maximizing Morning Light Exposure\"},\n",
    "    {\"start\": \"00:14:12\", \"end\": \"00:16:08\", \"topic\": \"Other Factors Influencing Circadian Rhythms\"},\n",
    "    {\"start\": \"00:16:08\", \"end\": \"00:24:31\", \"topic\": \"The Impact of Light on Sleep Quality\"},\n",
    "    {\"start\": \"00:24:31\", \"end\": \"00:28:00\", \"topic\": \"Napping & Non-Sleep Deep Rest\"},\n",
    "]\n",
    "\n",
    "# Convert to seconds\n",
    "for segment in timestamp_segments:\n",
    "    segment[\"start_sec\"] = hms_to_seconds(segment[\"start\"])\n",
    "    segment[\"end_sec\"] = hms_to_seconds(segment[\"end\"])\n",
    "\n",
    "# === Step 3: Download Transcript ===\n",
    "\n",
    "try:\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "except Exception as e:\n",
    "    print(f\" Error fetching transcript: {e}\")\n",
    "    transcript = []\n",
    "\n",
    "# === Step 4: Chunking ===\n",
    "\n",
    "chunked_output = []\n",
    "\n",
    "for segment in timestamp_segments:\n",
    "    chunk_lines = [\n",
    "        line[\"text\"] for line in transcript\n",
    "        if segment[\"start_sec\"] <= line[\"start\"] < segment[\"end_sec\"]\n",
    "    ]\n",
    "    full_text = \" \".join(chunk_lines).strip()\n",
    "\n",
    "    if full_text:\n",
    "        chunked_output.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"video_title\": video_title,\n",
    "            \"topic\": segment[\"topic\"],\n",
    "            \"start_time\": segment[\"start\"],\n",
    "            \"end_time\": segment[\"end\"],\n",
    "            \"text\": full_text\n",
    "        })\n",
    "\n",
    "# === Step 5: Save JSON ===\n",
    "\n",
    "\n",
    "output_path = os.path.abspath(os.path.join(\"..\", \"Chunks\", f\"{video_title.replace(' ', '_')}_{video_id}.json\"))\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunked_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\" Segmented chunks saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9a757f",
   "metadata": {},
   "source": [
    "Second video: Your Diet is Changing Your Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "077b3f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Segmented chunks saved to: c:\\Users\\alvar\\OneDrive\\文件\\Iron Hack\\ALMA-Chatbot\\Chunks\\Your_Diet_is_Changing_Your_Brain_NbymuYEEqlE.json\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions ===\n",
    "\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r\"v=([a-zA-Z0-9_-]{11})\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def hms_to_seconds(hms: str) -> float:\n",
    "    parts = list(map(int, hms.split(':')))\n",
    "    if len(parts) == 2:  # MM:SS format\n",
    "        m, s = parts\n",
    "        return m * 60 + s\n",
    "    elif len(parts) == 3:  # HH:MM:SS format\n",
    "        h, m, s = parts\n",
    "        return h * 3600 + m * 60 + s\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid time format: {hms}\")\n",
    "\n",
    "# Video Info and Timestamp Segments \n",
    "\n",
    "video = {\n",
    "    \"url\": \"https://www.youtube.com/watch?v=NbymuYEEqlE\",\n",
    "    \"Name\": \"Your Diet is Changing Your Brain\"\n",
    "}\n",
    "video_id = extract_video_id(video[\"url\"])\n",
    "video_title = video[\"Name\"]\n",
    "\n",
    "timestamp_segments_2 = [\n",
    "    {\"start\": \"00:00\", \"end\": \"00:30\", \"topic\": \"Ultra Processed Foods & Health\"},\n",
    "    {\"start\": \"00:30\", \"end\": \"01:19\", \"topic\": \"Mental Health Statistics\"},\n",
    "    {\"start\": \"01:19\", \"end\": \"01:52\", \"topic\": \"Examples of UPFs\"},\n",
    "    {\"start\": \"01:52\", \"end\": \"03:59\", \"topic\": \"Public Health Change\"},\n",
    "    {\"start\": \"03:59\", \"end\": \"04:47\", \"topic\": \"Challenges in Changing Public Behavior\"},\n",
    "    {\"start\": \"04:47\", \"end\": \"07:52\", \"topic\": \"Industry Influence & Misinformation\"},\n",
    "    {\"start\": \"07:52\", \"end\": \"09:22\", \"topic\": \"The Need for Systemic Change\"},\n",
    "    {\"start\": \"09:22\", \"end\": \"10:30\", \"topic\": \"Potential Solutions\"},\n",
    "]\n",
    "\n",
    "# Convert to seconds\n",
    "for segment in timestamp_segments_2:\n",
    "    segment[\"start_sec\"] = hms_to_seconds(segment[\"start\"])\n",
    "    segment[\"end_sec\"] = hms_to_seconds(segment[\"end\"])\n",
    "\n",
    "# === Step 3: Download Transcript ===\n",
    "\n",
    "try:\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "except Exception as e:\n",
    "    print(f\" Error fetching transcript: {e}\")\n",
    "    transcript = []\n",
    "\n",
    "# === Step 4: Chunking ===\n",
    "\n",
    "chunked_output = []\n",
    "\n",
    "for segment in timestamp_segments_2:\n",
    "    chunk_lines = [\n",
    "        line[\"text\"] for line in transcript\n",
    "        if segment[\"start_sec\"] <= line[\"start\"] < segment[\"end_sec\"]\n",
    "    ]\n",
    "    full_text = \" \".join(chunk_lines).strip()\n",
    "\n",
    "    if full_text:\n",
    "        chunked_output.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"video_title\": video_title,\n",
    "            \"topic\": segment[\"topic\"],\n",
    "            \"start_time\": segment[\"start\"],\n",
    "            \"end_time\": segment[\"end\"],\n",
    "            \"text\": full_text\n",
    "        })\n",
    "\n",
    "# === Step 5: Save JSON ===\n",
    "\n",
    "\n",
    "output_path = os.path.abspath(os.path.join(\"..\", \"Chunks\", f\"{video_title.replace(' ', '_')}_{video_id}.json\"))\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunked_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\" Segmented chunks saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be7501",
   "metadata": {},
   "source": [
    "Third video: Foods Control Our Moods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23478e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Segmented chunks saved to: c:\\Users\\alvar\\OneDrive\\文件\\Iron Hack\\ALMA-Chatbot\\Chunks\\Foods_Control_Our_Moods_Q4qWzbP0q7I.json\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions ===\n",
    "\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r\"v=([a-zA-Z0-9_-]{11})\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def hms_to_seconds(hms: str) -> float:\n",
    "    h, m, s = map(int, hms.split(':'))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "# Video Info and Timestamp Segments \n",
    "\n",
    "video = {\n",
    "    \"url\": \"https://www.youtube.com/watch?v=Q4qWzbP0q7I\",\n",
    "    \"Name\": \"Foods Control Our Moods\"\n",
    "}\n",
    "video_id = extract_video_id(video[\"url\"])\n",
    "video_title = video[\"Name\"]\n",
    "\n",
    "\n",
    "# Timestamps and topics for \"Foods Control Our Moods\"\n",
    "timestamp_segments_3 = [\n",
    "    {\"start\": \"00:00:00\", \"end\": \"00:02:30\", \"topic\": \"Huberman Lab Essentials; Emotions, Food & Nutrition\"},\n",
    "    {\"start\": \"00:02:30\", \"end\": \"00:03:38\", \"topic\": \"Attraction & Aversion\"},\n",
    "    {\"start\": \"00:03:38\", \"end\": \"00:06:31\", \"topic\": \"Vagus Nerve, Sugar\"},\n",
    "    {\"start\": \"00:06:31\", \"end\": \"00:08:54\", \"topic\": \"Gut Feelings, Hidden Sugars, Amino Acids\"},\n",
    "    {\"start\": \"00:08:54\", \"end\": \"00:12:57\", \"topic\": \"Dopamine, Craving, L-tyrosine\"},\n",
    "    {\"start\": \"00:12:57\", \"end\": \"00:16:12\", \"topic\": \"Serotonin, Carbohydrates\"},\n",
    "    {\"start\": \"00:16:12\", \"end\": \"00:19:12\", \"topic\": \"Omega-3s, Depression, SSRIs\"},\n",
    "    {\"start\": \"00:19:12\", \"end\": \"00:22:35\", \"topic\": \"Gut-Brain Axis, Gut Microbiome\"},\n",
    "    {\"start\": \"00:22:35\", \"end\": \"00:25:39\", \"topic\": \"Probiotics, Brain Fog, Fermented Foods, Saccharine Caution\"},\n",
    "    {\"start\": \"00:25:39\", \"end\": \"00:28:59\", \"topic\": \"Ketogenic Diet, Gut Microbiome, Diet Variability\"},\n",
    "    {\"start\": \"00:28:59\", \"end\": \"00:32:00\", \"topic\": \"Belief Effects & Key Takeaways\"},\n",
    "]\n",
    "\n",
    "# Convert to seconds\n",
    "for segment in timestamp_segments_3:\n",
    "    segment[\"start_sec\"] = hms_to_seconds(segment[\"start\"])\n",
    "    segment[\"end_sec\"] = hms_to_seconds(segment[\"end\"])\n",
    "\n",
    "# === Step 3: Download Transcript ===\n",
    "\n",
    "try:\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "except Exception as e:\n",
    "    print(f\" Error fetching transcript: {e}\")\n",
    "    transcript = []\n",
    "\n",
    "# === Step 4: Chunking ===\n",
    "\n",
    "chunked_output = []\n",
    "\n",
    "for segment in timestamp_segments_3:\n",
    "    chunk_lines = [\n",
    "        line[\"text\"] for line in transcript\n",
    "        if segment[\"start_sec\"] <= line[\"start\"] < segment[\"end_sec\"]\n",
    "    ]\n",
    "    full_text = \" \".join(chunk_lines).strip()\n",
    "\n",
    "    if full_text:\n",
    "        chunked_output.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"video_title\": video_title,\n",
    "            \"topic\": segment[\"topic\"],\n",
    "            \"start_time\": segment[\"start\"],\n",
    "            \"end_time\": segment[\"end\"],\n",
    "            \"text\": full_text\n",
    "        })\n",
    "\n",
    "# === Step 5: Save JSON ===\n",
    "\n",
    "\n",
    "output_path = os.path.abspath(os.path.join(\"..\", \"Chunks\", f\"{video_title.replace(' ', '_')}_{video_id}.json\"))\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunked_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\" Segmented chunks saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ddd5c9",
   "metadata": {},
   "source": [
    "Fourth video: 4 Small Habits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4cb2a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Segmented chunks saved to: c:\\Users\\alvar\\OneDrive\\文件\\Iron Hack\\ALMA-Chatbot\\Chunks\\4_Small_Habits_mMHNvy9pFj0.json\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions ===\n",
    "\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r\"v=([a-zA-Z0-9_-]{11})\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def hms_to_seconds(hms: str) -> float:\n",
    "    parts = list(map(int, hms.split(':')))\n",
    "    if len(parts) == 2:  # MM:SS format\n",
    "        m, s = parts\n",
    "        return m * 60 + s\n",
    "    elif len(parts) == 3:  # HH:MM:SS format\n",
    "        h, m, s = parts\n",
    "        return h * 3600 + m * 60 + s\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid time format: {hms}\")\n",
    "    \n",
    "# Video Info and Timestamp Segments \n",
    "\n",
    "video = {\n",
    "    \"url\": \"https://www.youtube.com/watch?v=mMHNvy9pFj0\",\n",
    "    \"Name\": \"4 Small Habits\"\n",
    "}\n",
    "video_id = extract_video_id(video[\"url\"])\n",
    "video_title = video[\"Name\"]\n",
    "\n",
    "# Define the timestamp segments for \"4 Small Habits\"\n",
    "timestamp_segments_4 = [\n",
    "    {\"start\": \"00:00\", \"end\": \"03:49\", \"topic\": \"Introduction\"},\n",
    "    {\"start\": \"03:49\", \"end\": \"14:54\", \"topic\": \"Dr. Chatterjee’s Four Pillars of Health\"},\n",
    "    {\"start\": \"14:54\", \"end\": \"42:09\", \"topic\": \"The First Pillar: Food\"},\n",
    "    {\"start\": \"42:09\", \"end\": \"56:53\", \"topic\": \"The Second Pillar: Movement\"},\n",
    "    {\"start\": \"56:53\", \"end\": \"1:05:55\", \"topic\": \"The Third Pillar: Sleep\"},\n",
    "    {\"start\": \"1:05:55\", \"end\": \"01:19:16\", \"topic\": \"The Fourth Pillar: Relaxation\"},\n",
    "    {\"start\": \"01:19:16\", \"end\": \"01:30:00\", \"topic\": \"Managing Stress as a Caregiver\"},  \n",
    "]\n",
    "\n",
    "# Convert to seconds\n",
    "for segment in timestamp_segments_4:\n",
    "    segment[\"start_sec\"] = hms_to_seconds(segment[\"start\"])\n",
    "    segment[\"end_sec\"] = hms_to_seconds(segment[\"end\"])\n",
    "\n",
    "# === Step 3: Download Transcript ===\n",
    "\n",
    "try:\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "except Exception as e:\n",
    "    print(f\" Error fetching transcript: {e}\")\n",
    "    transcript = []\n",
    "\n",
    "# === Step 4: Chunking ===\n",
    "\n",
    "chunked_output = []\n",
    "\n",
    "for segment in timestamp_segments_4:\n",
    "    chunk_lines = [\n",
    "        line[\"text\"] for line in transcript\n",
    "        if segment[\"start_sec\"] <= line[\"start\"] < segment[\"end_sec\"]\n",
    "    ]\n",
    "    full_text = \" \".join(chunk_lines).strip()\n",
    "\n",
    "    if full_text:\n",
    "        chunked_output.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"video_title\": video_title,\n",
    "            \"topic\": segment[\"topic\"],\n",
    "            \"start_time\": segment[\"start\"],\n",
    "            \"end_time\": segment[\"end\"],\n",
    "            \"text\": full_text\n",
    "        })\n",
    "\n",
    "# === Step 5: Save JSON ===\n",
    "\n",
    "\n",
    "output_path = os.path.abspath(os.path.join(\"..\", \"Chunks\", f\"{video_title.replace(' ', '_')}_{video_id}.json\"))\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunked_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\" Segmented chunks saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604e9a1",
   "metadata": {},
   "source": [
    "\n",
    "Fith video: Young Forever \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baddeafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Segmented chunks saved to: c:\\Users\\alvar\\OneDrive\\文件\\Iron Hack\\ALMA-Chatbot\\Chunks\\Young_Forever_gO_x3gnXBzg.json\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions ===\n",
    "\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r\"v=([a-zA-Z0-9_-]{11})\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def hms_to_seconds(hms: str) -> float:\n",
    "    parts = list(map(int, hms.split(':')))\n",
    "    if len(parts) == 2:  # MM:SS format\n",
    "        m, s = parts\n",
    "        return m * 60 + s\n",
    "    elif len(parts) == 3:  # HH:MM:SS format\n",
    "        h, m, s = parts\n",
    "        return h * 3600 + m * 60 + s\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid time format: {hms}\")\n",
    "    \n",
    "# Video Info and Timestamp Segments \n",
    "\n",
    "video = {\n",
    "    \"url\": \"https://www.youtube.com/watch?v=gO_x3gnXBzg\",\n",
    "    \"Name\": \"Young Forever\"\n",
    "}\n",
    "video_id = extract_video_id(video[\"url\"])\n",
    "video_title = video[\"Name\"]\n",
    "\n",
    "# Define the timestamp segments for \"Young Forever\"\n",
    "timestamp_segments_5 = [\n",
    "    {\"start\": \"00:00\", \"end\": \"05:21\", \"topic\": \"Welcome\"},\n",
    "    {\"start\": \"05:21\", \"end\": \"13:34\", \"topic\": \"Changing the Conversation About Aging\"},\n",
    "    {\"start\": \"13:34\", \"end\": \"24:05\", \"topic\": \"Dr. Vonda’s Journey From Cancer Nurse to Orthopedic Surgeon\"},\n",
    "    {\"start\": \"24:05\", \"end\": \"34:41\", \"topic\": \"The Incredible Power of Mobility on Your Health\"},\n",
    "    {\"start\": \"34:41\", \"end\": \"39:54\", \"topic\": \"How You Age Is In Your Control\"},\n",
    "    {\"start\": \"39:54\", \"end\": \"48:35\", \"topic\": \"Investing in Your Future Mobility\"},\n",
    "    {\"start\": \"48:35\", \"end\": \"1:03:59\", \"topic\": \"How to Start Your Fitness Journey: The FACE Acronym for Midlife Exercise\"},\n",
    "    {\"start\": \"1:03:59\", \"end\": \"1:08:00\", \"topic\": \"Debunking Myths About Joint Health\"},\n",
    "    {\"start\": \"1:08:00\", \"end\": \"1:20:00\", \"topic\": \"Addressing Arthritis Holistically\"}  \n",
    "]\n",
    "\n",
    "# Convert to seconds\n",
    "for segment in timestamp_segments_5:\n",
    "    segment[\"start_sec\"] = hms_to_seconds(segment[\"start\"])\n",
    "    segment[\"end_sec\"] = hms_to_seconds(segment[\"end\"])\n",
    "\n",
    "# === Step 3: Download Transcript ===\n",
    "\n",
    "try:\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "except Exception as e:\n",
    "    print(f\" Error fetching transcript: {e}\")\n",
    "    transcript = []\n",
    "\n",
    "# === Step 4: Chunking ===\n",
    "\n",
    "chunked_output = []\n",
    "\n",
    "for segment in timestamp_segments_5:\n",
    "    chunk_lines = [\n",
    "        line[\"text\"] for line in transcript\n",
    "        if segment[\"start_sec\"] <= line[\"start\"] < segment[\"end_sec\"]\n",
    "    ]\n",
    "    full_text = \" \".join(chunk_lines).strip()\n",
    "\n",
    "    if full_text:\n",
    "        chunked_output.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"video_title\": video_title,\n",
    "            \"topic\": segment[\"topic\"],\n",
    "            \"start_time\": segment[\"start\"],\n",
    "            \"end_time\": segment[\"end\"],\n",
    "            \"text\": full_text\n",
    "        })\n",
    "\n",
    "# === Step 5: Save JSON ===\n",
    "\n",
    "\n",
    "output_path = os.path.abspath(os.path.join(\"..\", \"Chunks\", f\"{video_title.replace(' ', '_')}_{video_id}.json\"))\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunked_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\" Segmented chunks saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58f6c1",
   "metadata": {},
   "source": [
    "Sixth video: Powerful Happiness Hacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a9f541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Segmented chunks saved to: c:\\Users\\alvar\\OneDrive\\文件\\Iron Hack\\ALMA-Chatbot\\Chunks\\Powerful_Happiness_Hacks_14-DJFPm1_4.json\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions ===\n",
    "\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r\"v=([a-zA-Z0-9_-]{11})\", url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def hms_to_seconds(hms: str) -> float:\n",
    "    parts = list(map(int, hms.split(':')))\n",
    "    if len(parts) == 2:  # MM:SS format\n",
    "        m, s = parts\n",
    "        return m * 60 + s\n",
    "    elif len(parts) == 3:  # HH:MM:SS format\n",
    "        h, m, s = parts\n",
    "        return h * 3600 + m * 60 + s\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid time format: {hms}\")\n",
    "    \n",
    "# Video Info and Timestamp Segments \n",
    "\n",
    "video = {\n",
    "    \"url\": \"https://www.youtube.com/watch?v=14-DJFPm1_4\",\n",
    "    \"Name\": \"Powerful Happiness Hacks\"\n",
    "}\n",
    "video_id = extract_video_id(video[\"url\"])\n",
    "video_title = video[\"Name\"]\n",
    "\n",
    "# Define the timestamp segments for \"Powerful Happiness Hacks\"\n",
    "timestamp_segments_6 = [\n",
    "    {\"start\": \"00:00\", \"end\": \"06:55\", \"topic\": \"Introduction\"},\n",
    "    {\"start\": \"06:55\", \"end\": \"14:21\", \"topic\": \"Yale’s Course on Happiness\"},\n",
    "    {\"start\": \"14:21\", \"end\": \"18:29\", \"topic\": \"Common Misconceptions About Happiness\"},\n",
    "    {\"start\": \"18:29\", \"end\": \"24:00\", \"topic\": \"Survival vs Thriving – Rewiring Happiness\"},\n",
    "    {\"start\": \"24:00\", \"end\": \"25:46\", \"topic\": \"Why Changing Circumstances Isn’t Enough\"},\n",
    "    {\"start\": \"25:46\", \"end\": \"34:01\", \"topic\": \"Money & Happiness\"},\n",
    "    {\"start\": \"34:01\", \"end\": \"40:08\", \"topic\": \"Spending Free Time to Feel Better\"},\n",
    "    {\"start\": \"40:08\", \"end\": \"44:06\", \"topic\": \"Slowing Down & Kindness\"},\n",
    "    {\"start\": \"44:06\", \"end\": \"49:01\", \"topic\": \"Happiness for Introverts & Extroverts\"},\n",
    "    {\"start\": \"49:01\", \"end\": \"54:37\", \"topic\": \"Helping Others & Joy\"},\n",
    "    {\"start\": \"54:37\", \"end\": \"1:08:48\", \"topic\": \"Finding Joy in Hard Moments\"},\n",
    "    {\"start\": \"1:08:48\", \"end\": \"1:10:49\", \"topic\": \"Everyday Habits for Instant Happiness\"},\n",
    "    {\"start\": \"1:10:49\", \"end\": \"1:16:44\", \"topic\": \"Mastering Self-Compassion\"},\n",
    "    {\"start\": \"1:16:44\", \"end\": \"1:20:08\", \"topic\": \"Happiness & a Better World\"},\n",
    "    {\"start\": \"1:20:08\", \"end\": \"1:30:00\", \"topic\": \"Happiness Homework & Final Thoughts\"}  \n",
    "]\n",
    "\n",
    "# Convert to seconds\n",
    "for segment in timestamp_segments_6:\n",
    "    segment[\"start_sec\"] = hms_to_seconds(segment[\"start\"])\n",
    "    segment[\"end_sec\"] = hms_to_seconds(segment[\"end\"])\n",
    "\n",
    "# === Step 3: Download Transcript ===\n",
    "\n",
    "try:\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "except Exception as e:\n",
    "    print(f\" Error fetching transcript: {e}\")\n",
    "    transcript = []\n",
    "\n",
    "# === Step 4: Chunking ===\n",
    "\n",
    "chunked_output = []\n",
    "\n",
    "for segment in timestamp_segments_6:\n",
    "    chunk_lines = [\n",
    "        line[\"text\"] for line in transcript\n",
    "        if segment[\"start_sec\"] <= line[\"start\"] < segment[\"end_sec\"]\n",
    "    ]\n",
    "    full_text = \" \".join(chunk_lines).strip()\n",
    "\n",
    "    if full_text:\n",
    "        chunked_output.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"video_title\": video_title,\n",
    "            \"topic\": segment[\"topic\"],\n",
    "            \"start_time\": segment[\"start\"],\n",
    "            \"end_time\": segment[\"end\"],\n",
    "            \"text\": full_text\n",
    "        })\n",
    "\n",
    "# === Step 5: Save JSON ===\n",
    "\n",
    "\n",
    "output_path = os.path.abspath(os.path.join(\"..\", \"Chunks\", f\"{video_title.replace(' ', '_')}_{video_id}.json\"))\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunked_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\" Segmented chunks saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b04eb1c",
   "metadata": {},
   "source": [
    "##### We enrich the chunks with OpenAI\n",
    "Send each chunk to GPT to assign more semantic tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdd0fcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing 4_Small_Habits_mMHNvy9pFj0.json\n",
      " Tagged and saved to: c:\\Users\\alvar\\OneDrive\\文件\\Iron Hack\\ALMA-Chatbot\\TaggedChunks\\4_Small_Habits_mMHNvy9pFj0_tagged.json\n",
      " Processing Foods_Control_Our_Moods_Q4qWzbP0q7I.json\n",
      " Tagged and saved to: c:\\Users\\alvar\\OneDrive\\文件\\Iron Hack\\ALMA-Chatbot\\TaggedChunks\\Foods_Control_Our_Moods_Q4qWzbP0q7I_tagged.json\n",
      " Processing Master_Your_Sleep_lIo9FcrljDk.json\n",
      " Tagged and saved to: c:\\Users\\alvar\\OneDrive\\文件\\Iron Hack\\ALMA-Chatbot\\TaggedChunks\\Master_Your_Sleep_lIo9FcrljDk_tagged.json\n",
      " Processing Powerful_Happiness_Hacks_14-DJFPm1_4.json\n",
      " Tagged and saved to: c:\\Users\\alvar\\OneDrive\\文件\\Iron Hack\\ALMA-Chatbot\\TaggedChunks\\Powerful_Happiness_Hacks_14-DJFPm1_4_tagged.json\n",
      " Processing Young_Forever_gO_x3gnXBzg.json\n",
      " Tagged and saved to: c:\\Users\\alvar\\OneDrive\\文件\\Iron Hack\\ALMA-Chatbot\\TaggedChunks\\Young_Forever_gO_x3gnXBzg_tagged.json\n",
      " Processing Your_Diet_is_Changing_Your_Brain_NbymuYEEqlE.json\n",
      " Tagged and saved to: c:\\Users\\alvar\\OneDrive\\文件\\Iron Hack\\ALMA-Chatbot\\TaggedChunks\\Your_Diet_is_Changing_Your_Brain_NbymuYEEqlE_tagged.json\n"
     ]
    }
   ],
   "source": [
    "# Load API key\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Your taxonomy of allowed tags\n",
    "TAXONOMY = [\n",
    "    \"sleep\", \"nutrition\", \"gut health\", \"mental health\", \"longevity\",\n",
    "    \"inflammation\", \"movement\", \"relaxation\", \"aging\", \"happiness\",\n",
    "    \"supplements\", \"diet\", \"stress\", \"preventive care\", \"food\",\n",
    "     \"exercise\",  \"health\", \"mindfulness\", \"emotional health\",\n",
    "    \"Circadian Rhythms\", \"Ultra Processed Foods\", \"cancer\"\n",
    "]\n",
    "\n",
    "# Define the path to the Chunks folder outside the Notebooks folder\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  \n",
    "CHUNKS_FOLDER = os.path.join(base_dir, \"Chunks\")  \n",
    "OUTPUT_FOLDER = os.path.join(base_dir, \"TaggedChunks\")  \n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Function to get tags from GPT using new API\n",
    "def tag_chunk(text, taxonomy):\n",
    "    prompt = f\"\"\"\n",
    "You are a health and wellness expert with deep knowledge in nutrition, mental and physical health, and healthy aging. \n",
    "You specialize in helping people improve their sleep, nutrition, and overall well-being.\n",
    "From the following text, extract only 1–3 most relevant topics from this list:\n",
    "\n",
    "{taxonomy}\n",
    "\n",
    "Text:\n",
    "\\\"\\\"\\\"\n",
    "{text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Respond only with a Python list of keywords, like: [\"sleep\", \"nutrition\"]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a professional health tagger for AI content.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return eval(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(\" Error tagging chunk:\", e)\n",
    "        return []\n",
    "\n",
    "# Loop over all JSON files in the Chunks folder\n",
    "for filename in os.listdir(CHUNKS_FOLDER):\n",
    "    if filename.endswith(\".json\"):\n",
    "        print(f\" Processing {filename}\")\n",
    "        path = os.path.join(CHUNKS_FOLDER, filename)\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for chunk in data:\n",
    "            tags = tag_chunk(chunk[\"text\"], TAXONOMY)\n",
    "            chunk[\"tags\"] = tags\n",
    "\n",
    "        # Save tagged output\n",
    "        output_path = os.path.join(OUTPUT_FOLDER, f\"{filename[:-5]}_tagged.json\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\" Tagged and saved to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d1dba1",
   "metadata": {},
   "source": [
    "##### Now we embed these chunks and push them into a vector DB\n",
    "\n",
    "Embedding + vector storage phase next (using Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f3844b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_11448\\3141271071.py:16: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Indexing 4_Small_Habits_mMHNvy9pFj0_tagged.json\n",
      " Indexing Foods_Control_Our_Moods_Q4qWzbP0q7I_tagged.json\n",
      " Indexing Master_Your_Sleep_lIo9FcrljDk_tagged.json\n",
      " Indexing Powerful_Happiness_Hacks_14-DJFPm1_4_tagged.json\n",
      " Indexing Young_Forever_gO_x3gnXBzg_tagged.json\n",
      " Indexing Your_Diet_is_Changing_Your_Brain_NbymuYEEqlE_tagged.json\n",
      "- All documents successfully indexed to Pinecone!\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = \"alma-index\"\n",
    "\n",
    "# Connect to Pinecone index\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    raise ValueError(f\" Index '{index_name}' not found.\")\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Folder where tagged chunks are saved\n",
    "TAGGED_FOLDER = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"TaggedChunks\"))\n",
    "\n",
    "# Loop through tagged JSONs and add to Pinecone\n",
    "for filename in os.listdir(TAGGED_FOLDER):\n",
    "    if filename.endswith(\".json\"):\n",
    "        print(f\" Indexing {filename}\")\n",
    "        filepath = os.path.join(TAGGED_FOLDER, filename)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            chunks = json.load(f)\n",
    "\n",
    "        docs = []\n",
    "        for chunk in chunks:\n",
    "            metadata = {\n",
    "                \"topic\": chunk.get(\"topic\"),\n",
    "                \"tags\": \", \".join(chunk.get(\"tags\", [])),\n",
    "                \"video_title\": chunk.get(\"video_title\"),\n",
    "                \"video_id\": chunk.get(\"video_id\"),\n",
    "                \"start_time\": chunk.get(\"start_time\"),\n",
    "                \"end_time\": chunk.get(\"end_time\")\n",
    "            }\n",
    "            docs.append(Document(page_content=chunk[\"text\"], metadata=metadata))\n",
    "\n",
    "        # Use the updated PineconeVectorStore (new SDK-compatible)\n",
    "        vectorstore = PineconeVectorStore.from_documents(\n",
    "            docs, embedding=embeddings, index_name=index_name\n",
    "        )\n",
    "\n",
    "print(\"- All documents successfully indexed to Pinecone!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b72530",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
